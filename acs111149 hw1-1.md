
# XGBoost 模型訓練與優化說明

## 實驗目的
本實驗目的是針對不平衡資料集進行有效的詐欺交易偵測，並透過 XGBoost 監督式學習模型進行分類，優化精準率（Precision）與召回率（Recall）以提升整體 F1 分數。

---

## 模型設計
本模型選用 **XGBoost（eXtreme Gradient Boosting）**，一種基於梯度提升樹（GBDT）的強大分類器，適用於非線性、不平衡、高維度特徵的場景，具備下列特點：

- 適合處理詐欺偵測等極度類別不平衡的問題。
- 支援 `scale_pos_weight` 進行樣本比例補償。
- 可透過調整閾值以優化 Precision/Recall。
- 訓練速度快、效能高且易於微調。

---

## 資料來源與預處理
- 資料集：Kaggle [`mlg-ulb/creditcardfraud`](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)
- 資料欄位：
  - `Time`：已移除，與模型預測無直接關係
  - `Amount`：使用 `StandardScaler` 進行標準化
  - `Class`：目標變數（0=正常，1=詐欺）
---

## 模型參數設計與意義

| 參數               | 設定值 | 說明 |
|--------------------|--------|------|
| `eval_metric`       | `'logloss'` | 損失函數選擇，用於二分類 |
| `random_state`      | `42`   | 固定隨機性以確保可重現性 |
| `scale_pos_weight`  | `neg / pos` | 平衡正負樣本比例（不平衡資料補償）|
| `max_depth`         | `10`   | 決策樹最大深度，防止過擬合 |
| `learning_rate`     | `0.2`  | 每次提升的學習步長 |
| `n_estimators`      | `200`  | 弱分類器（樹）的總數 |
| `subsample`         | `0.8`  | 每棵樹訓練時使用的樣本比例 |
| `colsample_bytree`  | `0.8`  | 每棵樹訓練時使用的特徵比例 |

---

## 閾值搜尋邏輯（Threshold Search）
訓練完模型後，對 `predict_proba` 預測結果進行閾值調整：

- 搜尋範圍：0.3 ~ 0.95（步進 0.01）
- 評估標準：必須同時滿足以下條件才更新最佳閾值：
  - `Precision ≥ 0.94`
  - `Recall ≥ 0.82`
  - `F1 Score > 目前最佳分數`

```python
if prec >= 0.94 and rec >= 0.82 and f1 > best_score:
    best_score = f1
    best_thresh = thresh
```

---


## 預期執行結果範例
XGBoost 最終優化 Evaluation (Threshold=0.67):
=============================================
         Accuracy: 0.9993
  Precision Score: 0.9512
     Recall Score: 0.8421
         F1 Score: 0.8937

Classification Report:
              precision    recall  f1-score   support

           0     0.9998    1.0000    0.9999     85296
           1     0.9512    0.8421    0.8937       147

    accuracy                         0.9993     85443
   macro avg     0.9755    0.9211    0.9468     85443
weighted avg     0.9993    0.9993    0.9993     85443