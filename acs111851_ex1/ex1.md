### 監督式學習

### 非監督式學習
指令程式碼我採用K-Means結合馬氏距離（Mahalanobis Distance）進行信用卡詐欺偵測資料集的異常偵測

由於信用卡詐騙資料集特性：此資料集高度不平衡，正常交易（Class=0）遠多於詐騙詐欺交易（Class=1）。特徵數量多（29個特徵，包括28個PCA降維後的特徵V1-V28和金額）。詐騙交易通常會導致異常值，與正常交易在特徵空間的分佈有顯著差異。

K-Means：將正常交易恐，假設詐欺詐欺交易會脫離正常交易的恐中心，從而偵測出詐欺詐欺資料的異常。

馬氏距離：馬氏距離則考慮了特徵間的協方差結構，相比歐幾里德距離更適合檢測多維資料中的異常值。它能夠捕捉資料分佈的形狀和圖形，對於信用卡詐騙這種高維度、不平衡資料特別有效。

參數調整的部分與參數細節影響模型表現：K-Means的恐慌數（K值）和、PCA的保留變異比例、樣本大小和異常檢測閾值會直接影響模型的準確率（ precision）、召回率（recall）和F1分數。這些參數可以透過網格搜尋（Grid Search）進行調優，以在高準確率和合理召回率之間取得平衡。

信用卡詐欺詐欺偵測的目的：高準確率：避免將正常交易誤判為詐欺（假積極），因為合理召回率：盡量偵測出所有詐欺詐欺交易（真積極），但不能過度追求召回率而詐欺準確率。方案碼中使用分數 = 0.85 * 精確度 + 0.15 * 召回作為評估指標，顯示更準確重視率，這與實際應用減少中誤報的需求一致。

方案碼中參數調整的詳細說明：

資料共享的部分 #data = data.drop(['Time'], axis=1) #data['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))

資料上先移除了時間欄位，時間欄位表示交易的時間，對於異常檢測跟欺騙模式識別無直接貢獻，且可能引入噪音。

影響：簡化特徵空間，聚焦於與欺騙相關的特徵（如V1-V28和Amount）。

標準化數量欄位： 原因：數量欄位的數值範圍與其他特徵（V1-V28，已由PCA標準化）不一致，標準化保證所有特徵的刻度統一，避免某些特徵對距離計算的過度影響。

調整方式：使用StandardScaler將Amount轉換為平均值0、標準差1的分佈。

影響：提高K-Means和馬氏距離計算的穩定性和準確性。

PCA 降維的操作 pca_variances = [0.96, 0.97, 0.98] pca = PCA(n_components=pca_var, random_state=RANDOM_SEED) x_train_pca = pca.fit_transform(x_train) x_test_train) x_test_pca.

參數： pca_variances=[0.96, 0.97, 0.98]：PCA 保留的變異比例，分別測試 96%、97%、98%。 n_components=pca_var：自動選擇主成分數量以保留指定的變異比例。

因為信用卡詐騙資料集共有29個特徵，高維度資料可能導致計算成本高且存在影響不大的特徵。 PCA透過保留變異方向來降低維度，減少雜訊並加速模型計算。我的方案中測試了幾個不同的變異比例（96%、97%、98%），目的是保留足夠的資訊並降低維度主要之間找到一個平衡點。

調整方式：選擇其中的變異比例（接近1）以保留大部分訊息，但避免過高（如0.99）以減少變異。透過網格搜尋測試不同的比例，評估對最終F1分數的影響。
