## 監督式學習

結果:
```
XGBoost Evaluation:
=============================================
         Accuracy: 0.9996839998595555
  Precision Score: 0.957983193277311
     Recall Score: 0.8382352941176471
         F1 Score: 0.8941176470588236
```
與範例比較：各項指標上升約0.015
```
Comparing eg Model and my Model:
=============================================
Accuracy Difference (my Model - eg Model): 0.000047
Precision Difference (my Model - eg Model): 0.016807
Recall Difference (my Model - eg Model): 0.014706
F1 Difference (my Model - eg Model): 0.015686
```
### 資料處理
在這次作業中我嘗試了多種不同的預處理方式如`SMOTE`、`SMOTEENN`等，
但對結果沒有提升，最後選擇不作過多處理，僅依照範例處理特定特徵。

### 模型選擇：`XGboost`
XGboost適合處理不平衡資料，而這次的信用卡詐騙資料集即為不平衡資料
### 調整的參數：
```
'max_depth': 6, #為樹的深度
'learning_rate': 0.13042046559796128, 
'n_estimators': 239, 
'subsample': 0.8734341721479638,
'colsample_bytree': 0.9573714080077769,
'scale_pos_weight': 0.39978732376580606, 'gamma': 0.14893186350468346
```
| 參數             | 說明                                                                 |
|----------------------|--------------------------------------------------------------------------|
| `max_depth`          | 樹的最大深度，控制模型的複雜度與擬合能力，較大值可能造成過擬合。        |
| `learning_rate`      | 學習率，控制每棵樹對整體模型的貢獻程度，數值越小訓練越穩定但需要更多迭代。 |
| `n_estimators`       | 要建立的樹的數量（即 boosting 回合數），與 `learning_rate` 互相影響。      |
| `subsample`          | 每棵樹訓練時的樣本抽樣比例，用來防止過擬合，值在 0~1 之間。              |
| `colsample_bytree`   | 每棵樹訓練時所使用的特徵比例，值越小會減少特徵干擾，提升泛化能力。        |
| `scale_pos_weight`   | 類別不平衡時的正樣本權重調整，用於提升模型對少數類別的識別能力。          |
| `gamma`              | 損失函數在進行節點分裂時的最小減益，值越大模型越保守，幫助避免過擬合。     |
### 調整方式
#### Bayesian Optimization
透過機率模型，選擇性地探索參數空間，比 Grid Search 更高效，尤其適合高維參數空間，效率高，收斂快。
使用`optuna`套件對以上參數進行多輪篩選，降低範圍

## 非監督式學習
結果：
```
========================================
         Accuracy: 0.8209459459459459
  Precision Score: 0.7745664739884393
     Recall Score: 0.9054054054054054
         F1 Score: 0.8348909657320872
```
### 資料處理
1. 使用`RobstScaler`對資料進行縮放，減少極端值的影響
2. 對資料進行欠抽樣`Under-sampling`，平衡正常樣本與詐欺樣本的數量
3. `PCA`特徵降維，透過投影去除多餘資訊
### 模型選擇: `KMeans`
將資料分成K群，讓資料彼此接近，只使用正常樣本來模擬正常分布。
若某群中詐欺樣本比例 高於平均值，則推定為異常群。
### 參數調整
```
'n_clusters': 8,
'init': 'k-means++',
'n_init': 39,
'tol': 0.0009019938907393534
```
同樣使用`Bayesian Optimization`
優化以下參數
| 參數             | 說明                                                                 |
|----------------------|--------------------------------------------------------------------------|
| `n_clusters`          | KMeans 的群數        |
| `init`      | 初始中心選法 |
| `n_init`       | 初始化次數      |
| `tol`          | 收斂容忍度              |
