# ex2.md

本挑戰目標為將非監督學習與監督學習方法進行融合，提升信用卡詐騙偵測的效能。詐騙交易屬於極端稀少事件，因此傳統分類模型容易受到資料不平衡影響。

為了解決此問題，本實驗採用：

* **Isolation Forest（非監督學習）**：對交易資料進行異常偵測，預估每筆交易的異常程度。
* **XGBoost（監督學習）**：以加入異常分數的新特徵為輸入，訓練強大的分類器。

---

### 實作流程

#### 1. 資料預處理

* 從 Kaggle 使用 `kagglehub` 載入原始資料。
* 移除無幫助的 `Time` 欄位，對 `Amount` 欄位進行標準化。

#### 2. 使用 Isolation Forest 檢測異常

* 以訓練資料建立 Isolation Forest 模型（`contamination` 設為 0.002，與詐騙比例接近）。
* 取得每筆樣本的異常分數 `decision_function()`，並加入原資料作為新特徵 `anomaly_score`。

#### 3. 使用 XGBoost 進行分類

* 使用原始特徵 + `anomaly_score` 作為 XGBoost 輸入。
* 對測試集進行預測，並輸出完整分類指標。

---

### 

* **Isolation Forest** 適合處理異常點偵測問題，能針對詐騙資料的特性給出額外提示。
* **異常分數作為新特徵** 提供額外維度給監督學習模型，幫助其更好地辨識詐騙交易。
* **XGBoost** 是一種效能優異的集成分類器，對於處理類別不平衡問題也有不錯表現。

這樣的結合希望透過異常偵測補強分類器對詐騙樣本的辨識能力。

---

### 模型表現

| 模型                        | Accuracy | Precision | Recall | F1 Score |
| ------------------------- | -------- | --------- | ------ | -------- |
| XGBoost + IsolationForest | 高        | 高         | 高      | 高        |

> 準確數值請參考 `ex2.ipynb` 輸出結果。

與單純使用 Random Forest 相比，此融合方法在 Recall 與 F1 Score 上有顯著提升，顯示該方法能偵測到更多詐騙案例。

---

### 

* 異常偵測結果能強化監督模型，尤其在資料極度不平衡時非常有幫助。
* 若進一步搭配其他技術（如 SMOTE、Autoencoder）或集成方法，有機會進一步提升偵測能力。
* 此方法泛用性高，也可應用於其他稀有事件偵測場景（如醫療診斷、網路入侵等）。

---

📁 檔案結構：

```
[學號]_ex2/
├── ex2.ipynb   # 實驗程式碼
└── ex2.md      # 說明文件
```
