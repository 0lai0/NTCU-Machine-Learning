### 模型選擇與參數說明

---

#### 監督式學習：`XGBClassifier`（XGBoost）

**選擇理由：**
- 原生支援類別不平衡處理，能有效提升對詐騙樣本（少數類別）的辨識能力。
- 結合梯度提升與決策樹，具備強大非線性建模與特徵擷取能力。
- 執行效率高，並可透過 `scale_pos_weight` 精準調整詐騙樣本的偵測權重。

**資料處裡：**
- 在進行監督式學習前，先透過 IsolationForest 非監督式異常偵測模型，針對訓練資料進行初步的異常篩選。
- 該模型能有效識別潛在的異常樣本，特別適用於 異常比例極低 的場景（如詐騙偵測）。
- 篩選結果（異常標記）進一步被轉換為一個 新的特徵欄位（是否異常），並合併至原始資料中，強化後續 XGBoost 模型對異常交易的辨識能力。

**參數設定：(採用挑戰一的最佳組合)**
| 參數名稱              | 說明 |
|-----------------------|------|
| `n_estimators=211`    | 樹的數量（弱分類器個數），增加可能提升效果，但會提高訓練成本 |
| `learning_rate=0.168601190206765`| 控制每棵樹對預測的影響，較小值能提升泛化能力但需更多樹 |
| `max_depth=9`         | 控制樹的最大深度，避免過擬合 |
| `subsample=0.9989914764140614`     | 每棵樹訓練使用的樣本比例，有助防止過擬合 |
| `colsample_bytree=0.7464806626828626` | 每棵樹訓練使用的特徵比例，降低特徵干擾 |
| `gamma=0.4911060467180274`         | 分裂節點所需的最小資訊增益，越大越保守 |
| `scale_pos_weight`    | 根據異常/正常樣本比例自動設定，用於平衡類別權重 |
| `min_child_weight=1`  | 限制葉節點最小樣本權重總和，避免學習到噪音 |
| `tree_method='hist'`  | 使用直方圖加速訓練，適合大資料集 |
| `eval_metric='logloss'` | 使用對數損失作為模型效能評估指標 |
| `random_state=42`     | 固定隨機種子，確保結果可重現 |

**實驗結果：**
# XGBClassifier Evaluation:
```
         Accuracy: 0.9996371850239341
  Precision Score: 0.9133858267716536
     Recall Score: 0.8529411764705882
         F1 Score: 0.8821292775665399

Classification Report:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     85307
           1       0.91      0.85      0.88       136

    accuracy                           1.00     85443
   macro avg       0.96      0.93      0.94     85443
weighted avg       1.00      1.00      1.00     85443
```