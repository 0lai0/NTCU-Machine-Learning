{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qo82p8G0N7hm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a020a90-2e81-4a82-b9da-ca7e5bbe5d9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NTCU-Machine-Learning'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 18 (delta 1), reused 0 (delta 0), pack-reused 16 (from 1)\u001b[K\n",
            "Receiving objects: 100% (18/18), 5.13 KiB | 1.03 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ggguuuaaannn/NTCU-Machine-Learning.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, precision_recall_curve\n",
        "\n",
        "# 下載資料\n",
        "path = kagglehub.dataset_download(\"mlg-ulb/creditcardfraud\")\n",
        "df = pd.read_csv(f\"{path}/creditcard.csv\")\n",
        "\n",
        "# 前處理\n",
        "df['Amount'] = StandardScaler().fit_transform(df[['Amount']])\n",
        "df = df.drop(columns=['Time'])\n",
        "X = df.drop(columns=['Class'])\n",
        "y = df['Class']\n",
        "\n",
        "# 資料切分與標準化\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Isolation Forest\n",
        "iso = IsolationForest(n_estimators=600, contamination=0.017, max_features=10, random_state=42)\n",
        "iso.fit(X_train_scaled)\n",
        "\n",
        "X_train_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
        "X_test_df = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
        "X_train_df[\"anomaly_score\"] = iso.decision_function(X_train_scaled)\n",
        "X_test_df[\"anomaly_score\"] = iso.decision_function(X_test_scaled)\n",
        "X_train_df[\"isolation_label\"] = (iso.predict(X_train_scaled) == -1).astype(int)\n",
        "X_test_df[\"isolation_label\"] = (iso.predict(X_test_scaled) == -1).astype(int)\n",
        "\n",
        "# XGBoost 訓練（精調）\n",
        "xgb = XGBClassifier(\n",
        "    max_depth=7,\n",
        "    n_estimators=710,\n",
        "    learning_rate=0.031,\n",
        "    subsample=0.91,\n",
        "    colsample_bytree=0.84,\n",
        "    scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),\n",
        "    eval_metric='logloss',\n",
        "    random_state=42\n",
        ")\n",
        "xgb.fit(X_train_df, y_train)\n",
        "\n",
        "# 預測 + 最佳 threshold\n",
        "y_prob = xgb.predict_proba(X_test_df)[:, 1]\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
        "f1_scores = 2 * precision * recall / (precision + recall + 1e-6)\n",
        "best_threshold = thresholds[np.argmax(f1_scores)]\n",
        "\n",
        "# 最終預測\n",
        "y_pred = (y_prob >= best_threshold).astype(int)\n",
        "\n",
        "# 評估\n",
        "print(\"\\nHybrid Model Evaluation:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Accuracy       : {accuracy_score(y_test, y_pred)}\")\n",
        "print(f\"Precision Score: {precision_score(y_test, y_pred)}\")\n",
        "print(f\"Recall Score   : {recall_score(y_test, y_pred)}\")\n",
        "print(f\"F1 Score       : {f1_score(y_test, y_pred)}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, digits=2))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zACL63pUP_MP",
        "outputId": "40e7dea5-4307-4be8-a054-9cd9ad67ebda"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hybrid Model Evaluation:\n",
            "==================================================\n",
            "Accuracy       : 0.9996839998595555\n",
            "Precision Score: 0.9658119658119658\n",
            "Recall Score   : 0.8308823529411765\n",
            "F1 Score       : 0.8932806324110671\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     85307\n",
            "           1       0.97      0.83      0.89       136\n",
            "\n",
            "    accuracy                           1.00     85443\n",
            "   macro avg       0.98      0.92      0.95     85443\n",
            "weighted avg       1.00      1.00      1.00     85443\n",
            "\n"
          ]
        }
      ]
    }
  ]
}