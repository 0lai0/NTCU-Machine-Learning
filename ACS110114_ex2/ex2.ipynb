{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7792c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import kagglehub\n",
    "\n",
    "# 設定隨機種子和參數\n",
    "RANDOM_SEED = 42\n",
    "TEST_SIZE = 0.3\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d034f306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用設備: cuda\n",
      "GPU名稱: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "# 設定隨機種子\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# 檢查GPU可用性\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'使用設備: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU名稱: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80723d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "載入數據集...\n",
      "數據預處理...\n",
      "詐欺交易: 492, 正常交易: 284315\n",
      "詐欺交易比例: 0.173%\n",
      "訓練集大小: (199364, 29)\n",
      "測試集大小: (85443, 29)\n"
     ]
    }
   ],
   "source": [
    "print(\"載入數據集...\")\n",
    "path = kagglehub.dataset_download(\"mlg-ulb/creditcardfraud\")\n",
    "data = pd.read_csv(f\"{path}/creditcard.csv\")\n",
    "data['Class'] = data['Class'].astype(int)\n",
    "\n",
    "# 數據預處理\n",
    "print(\"數據預處理...\")\n",
    "data = data.drop(['Time'], axis=1)\n",
    "\n",
    "# 標準化 Amount 欄位\n",
    "scaler_amount = StandardScaler()\n",
    "data['Amount'] = scaler_amount.fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "\n",
    "# 顯示數據分布\n",
    "fraud = data[data['Class'] == 1]\n",
    "nonfraud = data[data['Class'] == 0]\n",
    "print(f'詐欺交易: {len(fraud)}, 正常交易: {len(nonfraud)}')\n",
    "print(f'詐欺交易比例: {len(fraud)/(len(fraud) + len(nonfraud))*100:.3f}%')\n",
    "\n",
    "# 準備特徵和標籤\n",
    "X = data.drop(columns=['Class']).values\n",
    "y = data['Class'].values\n",
    "\n",
    "# 分割訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "# 標準化特徵\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"訓練集大小: {X_train_scaled.shape}\")\n",
    "print(f\"測試集大小: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28e734fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 步驟1: 使用 Isolation Forest 進行異常檢測 ===\n",
      "用於訓練 Isolation Forest 的正常交易數量: 10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isolation Forest 檢測到的異常數量 (訓練集): 4250\n",
      "Isolation Forest 檢測到的異常數量 (測試集): 1863\n"
     ]
    }
   ],
   "source": [
    "# 步驟1: 非監督學習 - Isolation Forest\n",
    "print(\"\\n=== 步驟1: 使用 Isolation Forest 進行異常檢測 ===\")\n",
    "\n",
    "# 使用正常交易數據訓練 Isolation Forest\n",
    "normal_data = X_train_scaled[y_train == 0]\n",
    "# 使用更多數據但設置合理的樣本數\n",
    "sample_size = min(10000, len(normal_data))  # 使用更多樣本\n",
    "normal_sample = normal_data[:sample_size]\n",
    "print(f\"用於訓練 Isolation Forest 的正常交易數量: {len(normal_sample)}\")\n",
    "\n",
    "# 訓練 Isolation Forest\n",
    "isolation_forest = IsolationForest(\n",
    "    contamination=0.02,  # 降低預期異常比例至2%，更貼近實際\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_estimators=200,    # 增加樹的數量提高穩定性\n",
    "    max_samples=1000,    # 固定樣本數\n",
    "    max_features=0.8,    # 使用80%的特徵\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "isolation_forest.fit(normal_data)\n",
    "\n",
    "# 獲取異常分數\n",
    "train_anomaly_scores = isolation_forest.decision_function(X_train_scaled)\n",
    "test_anomaly_scores = isolation_forest.decision_function(X_test_scaled)\n",
    "\n",
    "# 預測異常（-1為異常，1為正常）\n",
    "train_anomaly_pred = isolation_forest.predict(X_train_scaled)\n",
    "test_anomaly_pred = isolation_forest.predict(X_test_scaled)\n",
    "\n",
    "# 將異常分數轉換為特徵（分數越低越可能是異常）\n",
    "train_anomaly_features = train_anomaly_scores.reshape(-1, 1)\n",
    "test_anomaly_features = test_anomaly_scores.reshape(-1, 1)\n",
    "\n",
    "print(f\"Isolation Forest 檢測到的異常數量 (訓練集): {np.sum(train_anomaly_pred == -1)}\")\n",
    "print(f\"Isolation Forest 檢測到的異常數量 (測試集): {np.sum(test_anomaly_pred == -1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e8f0732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 步驟2: 組合原始特徵和異常分數特徵 ===\n",
      "增強後的特徵維度: 30\n"
     ]
    }
   ],
   "source": [
    "# 步驟2: 組合特徵\n",
    "print(\"\\n=== 步驟2: 組合原始特徵和異常分數特徵 ===\")\n",
    "\n",
    "# 將異常分數作為新特徵加入\n",
    "X_train_enhanced = np.concatenate([X_train_scaled, train_anomaly_features], axis=1)\n",
    "X_test_enhanced = np.concatenate([X_test_scaled, test_anomaly_features], axis=1)\n",
    "\n",
    "print(f\"增強後的特徵維度: {X_train_enhanced.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd124692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步驟3: 定義深度神經網路模型\n",
    "class FraudDetectionNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32]):  # 減少模型複雜度\n",
    "        super(FraudDetectionNet, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.4 if i < len(hidden_dims)-1 else 0.2)  # 遞減 dropout\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # 輸出層\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        # 移除 Sigmoid，在損失函數中處理\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecb1d671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 步驟3: 創建深度神經網路模型 ===\n",
      "模型結構:\n",
      "FraudDetectionNet(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=30, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.4, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "    (8): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.2, inplace=False)\n",
      "    (12): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "類別權重: [  0.50086423 289.77325581]\n",
      "正類權重: 57.85\n"
     ]
    }
   ],
   "source": [
    "# 創建模型\n",
    "input_dim = X_train_enhanced.shape[1]\n",
    "model = FraudDetectionNet(input_dim).to(device)\n",
    "\n",
    "print(f\"\\n=== 步驟3: 創建深度神經網路模型 ===\")\n",
    "print(f\"模型結構:\")\n",
    "print(model)\n",
    "\n",
    "# 計算類別權重以處理不平衡數據 - 調整權重策略\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "# 降低正類權重，避免過度預測詐欺\n",
    "pos_weight = torch.tensor([class_weights[1]/class_weights[0] * 0.1], dtype=torch.float32).to(device)  # 降低10倍\n",
    "\n",
    "print(f\"類別權重: {class_weights}\")\n",
    "print(f\"正類權重: {pos_weight.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb603647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義損失函數和優化器\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0c202a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 準備數據載入器\n",
    "train_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_train_enhanced).to(device),\n",
    "    torch.FloatTensor(y_train).to(device)\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_test_enhanced).to(device),\n",
    "    torch.FloatTensor(y_test).to(device)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "350f9544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 步驟4: 訓練深度神經網路模型 ===\n",
      "Epoch [10/100], Train Loss: 0.0558, Val Loss: 0.0974, F1: 0.8029\n",
      "Epoch [20/100], Train Loss: 0.0346, Val Loss: 0.1086, F1: 0.8058\n",
      "Epoch [30/100], Train Loss: 0.0283, Val Loss: 0.1234, F1: 0.8127\n",
      "Epoch [40/100], Train Loss: 0.0230, Val Loss: 0.1322, F1: 0.8169\n",
      "Epoch [50/100], Train Loss: 0.0210, Val Loss: 0.1372, F1: 0.8085\n",
      "早停於第 52 個 epoch\n"
     ]
    }
   ],
   "source": [
    "# 步驟4: 訓練模型\n",
    "print(f\"\\n=== 步驟4: 訓練深度神經網路模型 ===\")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_f1 = 0\n",
    "best_model_state = None\n",
    "patience_counter = 0\n",
    "patience = 20\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_train_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(batch_X).squeeze()\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        loss.backward()\n",
    "        # 梯度裁剪防止梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "    \n",
    "    # 驗證 - 使用動態閾值\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            outputs = model(batch_X).squeeze()\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            probs = torch.sigmoid(outputs)\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "    \n",
    "    # 計算最佳閾值和F1分數\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # 使用 PR 曲線找最佳閾值\n",
    "    precision, recall, thresholds = precision_recall_curve(all_labels, all_probs)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    current_f1 = f1_scores[best_idx]\n",
    "    \n",
    "    train_losses.append(epoch_train_loss / len(train_loader))\n",
    "    val_losses.append(val_loss / len(test_loader))\n",
    "    \n",
    "    # 保存最佳模型和早停\n",
    "    if current_f1 > best_f1:\n",
    "        best_f1 = current_f1\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {train_losses[-1]:.4f}, '\n",
    "              f'Val Loss: {val_losses[-1]:.4f}, F1: {current_f1:.4f}')\n",
    "    \n",
    "    # 早停\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"早停於第 {epoch+1} 個 epoch\")\n",
    "        break\n",
    "    \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "279cb442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳 F1 分數: 0.8198\n"
     ]
    }
   ],
   "source": [
    "# 載入最佳模型\n",
    "model.load_state_dict(best_model_state)\n",
    "print(f\"最佳 F1 分數: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59e09506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 步驟5: 模型評估 ===\n"
     ]
    }
   ],
   "source": [
    "# 步驟5: 評估模型\n",
    "print(f\"\\n=== 步驟5: 模型評估 ===\")\n",
    "\n",
    "def evaluation(y_true, y_pred, model_name=\"Model\"):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    print(f'\\n{model_name} 評估結果:')\n",
    "    print('===' * 20)\n",
    "    print(f' 準確率 (Accuracy): {accuracy:.4f}')\n",
    "    print(f' 精確率 (Precision): {precision:.4f}')\n",
    "    print(f' 召回率 (Recall): {recall:.4f}')\n",
    "    print(f' F1 分數: {f1:.4f}')\n",
    "    print(\"\\n詳細分類報告:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19724334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終預測\n",
    "model.eval()\n",
    "final_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, _ in test_loader:\n",
    "        outputs = model(batch_X).squeeze()\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        final_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "final_probs = np.array(final_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f3ccc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動態找出最佳閾值\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, final_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8ea6dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳閾值: 0.9863\n",
      "該閾值下的 F1 分數: 0.8085\n"
     ]
    }
   ],
   "source": [
    "# 選擇 F1 分數最高的閾值\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "best_threshold_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[best_threshold_idx]\n",
    "\n",
    "print(f\"最佳閾值: {optimal_threshold:.4f}\")\n",
    "print(f\"該閾值下的 F1 分數: {f1_scores[best_threshold_idx]:.4f}\")\n",
    "\n",
    "# 使用最佳閾值進行預測\n",
    "final_preds = (final_probs > optimal_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c73d2407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 閾值優化和多種評估 ===\n",
      "Average Precision Score: 0.7498\n",
      "最佳 F1 閾值: 0.9863 (F1: 0.8085)\n",
      "高精確率閾值: 0.9997\n",
      "平衡閾值: 0.9564\n"
     ]
    }
   ],
   "source": [
    "# 新增：閾值優化和多種評估方法\n",
    "print(f\"\\n=== 閾值優化和多種評估 ===\")\n",
    "\n",
    "# 1. PR曲線分析\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, final_probs)\n",
    "ap_score = average_precision_score(y_test, final_probs)\n",
    "\n",
    "# 找到最佳 F1 閾值\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "best_f1_idx = np.argmax(f1_scores)\n",
    "best_f1_threshold = thresholds[best_f1_idx]\n",
    "\n",
    "# 找到精確率90%時的閾值（高精確率策略）\n",
    "high_precision_idx = np.where(precision >= 0.9)[0]\n",
    "if len(high_precision_idx) > 0:\n",
    "    high_precision_threshold = thresholds[high_precision_idx[0]]\n",
    "else:\n",
    "    high_precision_threshold = 0.9\n",
    "\n",
    "# 找到召回率80%時的閾值（平衡策略）\n",
    "balanced_recall_idx = np.where(recall >= 0.8)[0]\n",
    "if len(balanced_recall_idx) > 0:\n",
    "    balanced_threshold = thresholds[balanced_recall_idx[-1]]\n",
    "else:\n",
    "    balanced_threshold = 0.3\n",
    "\n",
    "print(f\"Average Precision Score: {ap_score:.4f}\")\n",
    "print(f\"最佳 F1 閾值: {best_f1_threshold:.4f} (F1: {f1_scores[best_f1_idx]:.4f})\")\n",
    "print(f\"高精確率閾值: {high_precision_threshold:.4f}\")\n",
    "print(f\"平衡閾值: {balanced_threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "569dbeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "最佳F1 (閾值=0.9863):\n",
      "  準確率: 0.9994, 精確率: 0.8496, 召回率: 0.7635, F1: 0.8043\n",
      "\n",
      "高精確率 (閾值=0.9997):\n",
      "  準確率: 0.9988, 精確率: 0.9057, 召回率: 0.3243, F1: 0.4776\n",
      "\n",
      "平衡策略 (閾值=0.9564):\n",
      "  準確率: 0.9992, 精確率: 0.7712, 召回率: 0.7973, F1: 0.7841\n",
      "\n",
      "預設0.5 (閾值=0.5000):\n",
      "  準確率: 0.9988, 精確率: 0.6080, 召回率: 0.8176, F1: 0.6974\n"
     ]
    }
   ],
   "source": [
    "# 2. 使用不同閾值進行評估\n",
    "thresholds_to_test = [best_f1_threshold, high_precision_threshold, balanced_threshold, 0.5]\n",
    "threshold_names = ['最佳F1', '高精確率', '平衡策略', '預設0.5']\n",
    "\n",
    "results_comparison = []\n",
    "for i, (threshold, name) in enumerate(zip(thresholds_to_test, threshold_names)):\n",
    "    preds = (final_probs > threshold).astype(int)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    prec = precision_score(y_test, preds, zero_division=0)\n",
    "    rec = recall_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    \n",
    "    results_comparison.append({\n",
    "        '策略': name,\n",
    "        '閾值': threshold,\n",
    "        '準確率': acc,\n",
    "        '精確率': prec,\n",
    "        '召回率': rec,\n",
    "        'F1分數': f1\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{name} (閾值={threshold:.4f}):\")\n",
    "    print(f\"  準確率: {acc:.4f}, 精確率: {prec:.4f}, 召回率: {rec:.4f}, F1: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b09dfe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 不同閾值策略比較 ===\n",
      "      策略      閾值     準確率     精確率     召回率    F1分數\n",
      "0   最佳F1  0.9863  0.9994  0.8496  0.7635  0.8043\n",
      "1   高精確率  0.9997  0.9988  0.9057  0.3243  0.4776\n",
      "2   平衡策略  0.9564  0.9992  0.7712  0.7973  0.7841\n",
      "3  預設0.5  0.5000  0.9988  0.6080  0.8176  0.6974\n",
      "\n",
      "=== Isolation Forest 單獨結果比較 ===\n",
      "\n",
      "Isolation Forest (單獨) 評估結果:\n",
      "============================================================\n",
      " 準確率 (Accuracy): 0.9790\n",
      " 精確率 (Precision): 0.0590\n",
      " 召回率 (Recall): 0.7432\n",
      " F1 分數: 0.1094\n",
      "\n",
      "詳細分類報告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     85295\n",
      "           1       0.06      0.74      0.11       148\n",
      "\n",
      "    accuracy                           0.98     85443\n",
      "   macro avg       0.53      0.86      0.55     85443\n",
      "weighted avg       1.00      0.98      0.99     85443\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9790386573505144,\n",
       " 0.059044551798174985,\n",
       " 0.7432432432432432,\n",
       " 0.10939830929885629)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 創建比較表格\n",
    "comparison_df = pd.DataFrame(results_comparison)\n",
    "print(f\"\\n=== 不同閾值策略比較 ===\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# 比較 Isolation Forest 單獨的結果\n",
    "iso_pred_binary = (test_anomaly_pred == -1).astype(int)\n",
    "print(f\"\\n=== Isolation Forest 單獨結果比較 ===\")\n",
    "evaluation(y_test, iso_pred_binary, \"Isolation Forest (單獨)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tetris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
