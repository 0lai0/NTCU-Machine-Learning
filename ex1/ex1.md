本程式碼使用 XGBoost，由於非詐騙樣本和詐騙樣本高度不平衡，僅保留詐騙樣本的 500 倍非詐騙樣本，最後與詐騙樣本合併並打亂

使用 XGBoost 模型進行訓練，設定如下：

scale_pos_weight=500：平衡正負樣本權重

n_estimators=300：建立 300 棵決策樹

max_depth=6：每棵樹最大深度為 6

subsample=0.8：每次訓練用 80% 訓練樣本

colsample_bytree=0.8：每棵樹用 80% 特徵

閾值設定 (threshold = 0.89)：
若預測機率 ≥ 0.89，則判定為詐騙

我認為影響最大的是閾值設定的部分，訂為0.89有助於避免誤判，而500倍非詐騙樣本幾乎和原始數據相同，但還是放了上去，scale_pos_weight影響也很大，數字越大越傾向於抓取詐騙

非監督的模型則是使用lof，並使用使用非詐騙樣本20000個訓練

經過多次測試，將n_neighbors設為30, contamination設為0.0003，會有最好的分數，雖然已經刻意調低contamination讓其偏向保守，但誤判率還是很高
