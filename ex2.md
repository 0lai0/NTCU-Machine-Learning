
# ğŸ›¡ï¸ Credit Card Fraud Detection using Hybrid Unsupervised and Supervised Learning

æœ¬å°ˆæ¡ˆå±•ç¤ºå¦‚ä½•çµåˆ **éç›£ç£å¼å­¸ç¿’ï¼ˆIsolation Forestã€PCAï¼‰** èˆ‡ **ç›£ç£å¼å­¸ç¿’ï¼ˆXGBoostï¼‰**ï¼Œæå‡ä¿¡ç”¨å¡è©æ¬ºåµæ¸¬çš„æº–ç¢ºç‡èˆ‡æ•ˆèƒ½ã€‚

è³‡æ–™ä¾†æºï¼š[Kaggle - Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)

---

## ğŸ“Œ å°ˆæ¡ˆæ¶æ§‹æ¦‚è¿°

æˆ‘å€‘å°‡æµç¨‹åˆ†ç‚ºä»¥ä¸‹å¹¾å€‹æ­¥é©Ÿï¼š

1. **è³‡æ–™å‰è™•ç†**
2. **æ¨™æº–åŒ–ç‰¹å¾µ**
3. **ä½¿ç”¨ Isolation Forest æ“·å–ç•°å¸¸åˆ†æ•¸**
4. **ä½¿ç”¨ PCA é™ç¶­æå–ç‰¹å¾µ**
5. **åˆä½µç‰¹å¾µ**
6. **XGBoost å»ºæ¨¡èˆ‡è©•ä¼°**

---

## ğŸ“¦ ä½¿ç”¨å¥—ä»¶

```bash
pip install pandas numpy scikit-learn xgboost kagglehub
```

---

## ğŸ“ è³‡æ–™è¼‰å…¥èˆ‡å‰è™•ç†

```python
import pandas as pd
from sklearn.preprocessing import StandardScaler

import kagglehub
path = kagglehub.dataset_download("mlg-ulb/creditcardfraud")
data = pd.read_csv(f"{path}/creditcard.csv")

data['Class'] = data['Class'].astype(int)
data = data.drop(['Time'], axis=1)
data['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))
```

---

## ğŸ” éç›£ç£å¼å­¸ç¿’éšæ®µï¼ˆç‰¹å¾µæ“´å……ï¼‰

### 1. **Isolation Forest**

```python
from sklearn.ensemble import IsolationForest

iso_forest = IsolationForest(contamination=0.0017, random_state=42)
iso_forest.fit(X_train_std)
```

| åƒæ•¸åç¨± | èªªæ˜ |
|----------|------|
| `contamination` | è¨­å®šç•°å¸¸æ¨£æœ¬çš„æ¯”ä¾‹ï¼ˆæ ¹æ“šè©æ¬ºç‡ï¼‰ |
| `random_state` | ç¢ºä¿å¯é‡ç¾æ€§ |

### 2. **PCA é™ç¶­**

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=10, random_state=42)
pca.fit(X_train_std)
```

| åƒæ•¸åç¨± | èªªæ˜ |
|----------|------|
| `n_components` | æå–å‰ 10 å€‹ä¸»æˆåˆ† |
| `random_state` | ç¢ºä¿å¯é‡ç¾æ€§ |

---

## ğŸ§  ç›£ç£å¼å­¸ç¿’éšæ®µï¼ˆæ¨¡å‹è¨“ç·´ï¼‰

### åˆä½µç‰¹å¾µ

```python
X_train_enhanced = np.hstack((X_train_std, train_anomaly_scores, X_train_pca))
X_test_enhanced = np.hstack((X_test_std, test_anomaly_scores, X_test_pca))
```

### è¨“ç·´ **XGBoost** æ¨¡å‹

```python
from xgboost import XGBClassifier

xgb_model = XGBClassifier(
    colsample_bytree=1.0,
    learning_rate=0.1,
    max_depth=6,
    n_estimators=200,
    subsample=0.8,
    scale_pos_weight=2.5,
    eval_metric='logloss',
    tree_method='hist',
    random_state=42
)
xgb_model.fit(X_train_enhanced, y_train)
```

| åƒæ•¸åç¨± | èªªæ˜ |
|------------|------|
| `learning_rate` | æ§åˆ¶æ¯æ£µæ¨¹å°æœ€çµ‚çµæœçš„è²¢ç» |
| `max_depth` | é™åˆ¶æ¨¹çš„æ·±åº¦é˜²æ­¢éæ“¬åˆ |
| `n_estimators` | å»ºç«‹æ¨¹çš„æ•¸é‡ |
| `subsample` | æ¯æ£µæ¨¹çš„è¨“ç·´æ¨£æœ¬æ¯”ä¾‹ |
| `scale_pos_weight` | è§£æ±ºé¡åˆ¥ä¸å¹³è¡¡å•é¡Œ |
| `tree_method` | ä½¿ç”¨ç›´æ–¹åœ–åŠ é€Ÿæ³•è¨“ç·´ |
| `eval_metric` | è©•ä¼°æ–¹å¼ï¼šlogloss |

---

## ğŸ¯ æ¨¡å‹é æ¸¬èˆ‡è©•ä¼°

```python
from sklearn.metrics import classification_report

y_prob = xgb_model.predict_proba(X_test_enhanced)[:, 1]
y_pred_custom = (y_prob > 0.43).astype(int)
```

### è©•ä¼°å‡½æ•¸

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def evaluation(y_true, y_pred, model_name="Model"):
    print(f'\n{model_name} Evaluation:')
    print('===' * 15)
    print('         Accuracy:', accuracy_score(y_true, y_pred))
    print('  Precision Score:', precision_score(y_true, y_pred))
    print('     Recall Score:', recall_score(y_true, y_pred))
    print('         F1 Score:', f1_score(y_true, y_pred))
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred))
```

### ğŸ“Š ç¯„ä¾‹çµæœ

| æŒ‡æ¨™ | åˆ†æ•¸ |
|------|------|
| **Accuracy** | 0.9997 |
| **Precision** | 0.9365 |
| **Recall** | 0.8676 |
| **F1 Score** | 0.9008 |

---

## çµè«–èˆ‡å„ªå‹¢

æœ¬å°ˆæ¡ˆæˆåŠŸçµåˆéç›£ç£èˆ‡ç›£ç£å­¸ç¿’æŠ€è¡“ï¼Œé”åˆ°é«˜æ•ˆèƒ½è©æ¬ºåµæ¸¬ã€‚

**å„ªå‹¢åŒ…æ‹¬ï¼š**

- æ•´åˆå¤šç¨®æ¨¡å‹ä»¥å¼·åŒ–è¾¨è­˜åŠ›
- ç‰¹å¾µçµ„åˆå…·å‚™ä»£è¡¨æ€§èˆ‡éˆæ´»æ€§
- é©ç”¨æ–¼å…¶ä»–ç•°å¸¸æª¢æ¸¬å ´æ™¯

---


