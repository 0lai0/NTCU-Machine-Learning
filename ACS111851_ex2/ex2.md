# 融合監督與非監督學習以提升預測效果
目標結合非監督式學習方法（如 Isolation Forest）與監督式分類器（如 XGBoost）
以在高度不平衡的信用卡詐騙偵測任務中提升模型的預測表現
### 方法概覽
流程步驟	       技術	                      功能  
資料前處理	  標準化（StandardScaler）	      提升模型穩定性與收斂速度   
非監督學習	  Isolation Forest	            初步標註潛在異常資料點，強化資料集的欺詐資訊密度   
特徵融合   	將異常分數加入特徵空間	        擴充特徵維度，引入非監督知識   
資料重取樣	  SMOTE（合成少數類）	          解決類別不平衡問題   
監督學習	    XGBoost + RandomizedSearchCV	高效的樹模型分類器，透過隨機搜尋優化超參數   
評估與閾值優化	動態閾值調整	平衡 precision / recall，提升實用性   
### 資料預處理
時間欄位剔除（Time）：
此欄位與詐騙行為關聯性不明，且存在高度偏態，因此這邊移除了該欄位data。
### Isolation Forest（非監督式異常偵測）
為一種基於隨機劃分的非監督式異常偵測方法，特別適合處理高維數據。
它不需標籤資料，能有效挖掘潛在異常點，對於類別極度不平衡（如詐騙率<0.2%）的場景極具價值。
### 使用方式與參數邏輯：
contamination=0.005：
表示預期 0.5% 為異常點，與原始詐騙樣本比例接近（約 0.172%）。
n_jobs=-1：多執行緒加速。
random_state=42：確保可重現。
### 異常分數整合方式：
使用 decision_function() 得到每筆資料的異常程度（越低越異常）
取 前 5% 的異常樣本（分數最低） 進行標註與後續處理，可提升資料集中詐騙樣本比例。
將 iso_score 作為 額外特徵欄位 合併進原始特徵空間中，提供後續分類模型額外的非監督信號。
### XGBoost + RandomizedSearchCV（監督分類 + 超參數調優）
### XGBoost特性
適合處理非線性、高維稀疏資料，並內建針對不平衡資料的處理方式（scale_pos_weight）。
支援早停、內建特徵重要性排序、分布式訓練等功能，效能與準確率兼具。
### 嘗試隨機搜尋參數設計部分
這邊原本使用了隨機搜尋參數設計，來找尋最佳參數，後來跑了幾輪結果不盡理想
原始設定範圍內無法跑出最佳結果，只能近似於目標結果
所以只做了參考，基於之前跑的結果又重新手動調整參數，目的在調整出能超過結果的最佳參數
### 嘗試SMOTE部分
本來嘗試使用SMOTE去平衡資料集但多次出來結果沒有比較好
所以後來就刪除SMOTE平衡部分，改採用scale_pos_weight 權重調整
因為此資料集中詐騙樣本數較少，dataset也呈現高度不平衡的現象
所以調整設置了scale_pos_weight參數，提高詐騙樣本在資料集中的重要性權重
### 最後使用的參數設定
參數	             設定值                參數定義  
n_estimators	    [500]	       提供足夠學習輪數以擬合複雜決策邊界    
max_depth	        [7]	         控制模型複雜度，過深可能過擬合    
learning_rate	    [0.3]	       探索不同收斂速度，避免 overshoot     
scale_pos_weight	[6]          對應類別不平衡程度，用以提高詐騙樣本的重要性權重    

### 閾值調整
預設分類閾值為 0.5，對於偏態資料極度不適用，會導致 recall 過低  
這邊改用0.4為設定值
特別是在銀行系統中，高 precision 代表較少誤報，低誤報率在實務上更重要
### 實驗總結與融合效益
IF + SMOTE + Tuned XGB	提升	0.94↑	較高	  
利用非監督特徵強化、資料平衡與超參數調整  
透過 Isolation Forest 初步篩選高風險交易，再透過 SMOTE 補強少數類數據、XGBoost 對複雜規則擬合與閾值微調    

