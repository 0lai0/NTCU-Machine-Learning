# 融合監督與非監督學習以提升預測效果
目標結合非監督式學習方法（如 Isolation Forest）與監督式分類器（如 XGBoost）
以在高度不平衡的信用卡詐騙偵測任務中提升模型的預測表現
### 方法概覽
流程步驟	技術	目的
資料前處理	標準化（StandardScaler）	提升模型穩定性與收斂速度  
非監督學習	Isolation Forest	初步標註潛在異常資料點，強化資料集的欺詐資訊密度  
特徵融合	將異常分數加入特徵空間	擴充特徵維度，引入非監督知識  
資料重取樣	SMOTE（合成少數類）	解決類別不平衡問題  
監督學習	XGBoost + RandomizedSearchCV	高效的樹模型分類器，透過隨機搜尋優化超參數  
評估與閾值優化	動態閾值調整	平衡 precision / recall，提升實用性  
### 資料預處理
時間欄位剔除（Time）：
此欄位與詐騙行為關聯性不明，且存在高度偏態，因此移除。
### Isolation Forest（非監督式異常偵測）
為一種基於隨機劃分的非監督式異常偵測方法，特別適合處理高維數據。
它不需標籤資料，能有效挖掘潛在異常點，對於類別極度不平衡（如詐騙率<0.2%）的場景極具價值。
### 使用方式與參數邏輯：
contamination=0.005：
表示預期 0.5% 為異常點，與原始詐騙樣本比例接近（約 0.172%）。
n_jobs=-1：多執行緒加速。
random_state=42：確保可重現。
### 異常分數整合方式：
使用 decision_function() 得到每筆資料的異常程度（越低越異常）
取 前 5% 的異常樣本（分數最低） 進行標註與後續處理，可提升資料集中詐騙樣本比例。
將 iso_score 作為 額外特徵欄位 合併進原始特徵空間中，提供後續分類模型額外的非監督信號。
### SMOTE 過採樣（監督式資料強化）
因為原始資料中詐騙類別極少（<0.2%），導致監督模型學習偏向負類
所以採用 SMOTE 來做平衡與優化
SMOTE 透過向量內插創建合成樣本，能有效避免過擬合與訊息重複，且不會造成資料「複製偏差」。
### 參數設定：
sampling_strategy=0.2：
表示將少數類比例提昇至 20%，雖非完全平衡，但能有效兼顧 precision 與 recall。
### XGBoost + RandomizedSearchCV（監督分類 + 超參數調優）
### XGBoost特性
適合處理非線性、高維稀疏資料，並內建針對不平衡資料的處理方式（scale_pos_weight）。
支援早停、內建特徵重要性排序、分布式訓練等功能，效能與準確率兼具。
### 隨機搜尋參數設計邏輯：
參數	設定範圍	理由
n_estimators	[500, 600]	提供足夠學習輪數以擬合複雜決策邊界  
max_depth	[6, 9, 12]	控制模型複雜度，過深可能過擬合  
learning_rate	[0.01, 0.05, 0.1]	探索不同收斂速度，避免 overshoot  
subsample	[0.8, 0.9, 1.0]	控制每棵樹的訓練樣本比例，降低過擬合  
colsample_bytree	[0.8, 0.9, 1.0]	降低特徵偏好，提高泛化能力  
scale_pos_weight	[30, 50, 70]	對應類別不平衡程度，用以提高詐騙樣本的重要性權重  

使用 scoring='f1' 最為搜尋目標，因為在詐騙任務中需兼顧 precision 與 recall。  

### 閾值調整
預設分類閾值為 0.5，對於偏態資料極度不適用，會導致 recall 過低。
特別是在銀行系統中，高 precision 代表較少誤報，低誤報率是實務首要考量
### 策略與參數選擇邏輯：
探索閾值範圍：0.5 ~ 0.9（每 0.05 為單位）
優先條件：precision >= 0.94
若符合 precision 門檻，選取最高 f1-score 對應的閾值。

### 實驗總結與融合效益
原始 XGBoost未做樣本強化與異常特徵  
IF + SMOTE + Tuned XGB	提升	0.94↑	較高	  
利用非監督特徵強化、資料平衡與超參數調整  
透過 Isolation Forest 初步篩選高風險交易，再透過 SMOTE 補強少數類數據、XGBoost 對複雜規則擬合與閾值微調    

