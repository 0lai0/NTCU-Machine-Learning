# -*- coding: utf-8 -*-
"""ex1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14KYfWZhYZvOAGvIwNToAKZd61Lhi6Lp2
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import kagglehub

# general setting. do not change TEST_SIZE
RANDOM_SEED = 42
TEST_SIZE = 0.3

# load dataset（from kagglehub）
path = kagglehub.dataset_download("mlg-ulb/creditcardfraud")
data = pd.read_csv(f"{path}/creditcard.csv")
data['Class'] = data['Class'].astype(int)

# prepare data
data = data.drop(['Time'], axis=1)
data['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))

fraud = data[data['Class'] == 1]
nonfraud = data[data['Class'] == 0]
print(f'Fraudulent:{len(fraud)}, non-fraudulent:{len(nonfraud)}')
print(f'the positive class (frauds) percentage: {len(fraud)}/{len(fraud) + len(nonfraud)} ({len(fraud)/(len(fraud) + len(nonfraud))*100:.3f}%)')

# Extract features and labels
X = np.asarray(data.drop(columns=['Class']))
y = np.asarray(data['Class'])

# Split the dataset into training and testing sets (with stratification)
x_train, x_test, y_train, y_test = train_test_split(
   X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=y
)

scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# 從訓練資料中取出 1000 筆正常樣本與 100 筆詐欺樣本，混合成分群訓練集
n_x_train = x_train[y_train == 0][:1000]
f_x_train = x_train[y_train == 1][:100]
mix_x_train = np.vstack([n_x_train, f_x_train])

# 設定要搜尋的分群數 k 範圍（2~10）
K_RANGE = range(2, 11)
scores = []
for k in K_RANGE:
    # 建立 KMeans 模型，指定分群數與相關超參數
    kmeans = KMeans(
        n_clusters=k,
        init='k-means++',
        n_init=10,           # 初始化次數
        max_iter=500,        # 最大迭代次數
        tol=1e-4,            # 收斂容忍度
        random_state=RANDOM_SEED,
        algorithm='elkan'    # 使用 elkan 演算法加速
    )
    # 用混合樣本訓練 KMeans
    kmeans.fit(mix_x_train)
    # 計算 silhouette score 作為分群品質指標
    score = silhouette_score(mix_x_train, kmeans.labels_)
    scores.append(score)

# 找出 silhouette score 最高時的 k 值
optimal_k = np.argmax(scores) + K_RANGE.start
# 用最佳 k 重新訓練 KMeans，並可調整超參數
kmeans = KMeans(
    n_clusters=optimal_k,
    init='k-means++',
    n_init=20,           # 增加初始化次數
    max_iter=700,        # 增加最大迭代次數
    tol=1e-4,
    random_state=RANDOM_SEED,
)
kmeans.fit(mix_x_train)

# 將測試資料分群，得到預測標籤
y_pred_test = kmeans.predict(x_test)

# 對齊分群標籤與真實標籤，讓每個群對應到最多的真實類別
# 這樣才能正確計算分類指標

def align_labels(y_true, y_pred, n_clusters):
   labels = np.zeros_like(y_pred)
   for i in range(n_clusters):
       mask = (y_pred == i)
       if np.sum(mask) > 0:
           # 對每個群，找出該群中最多的真實標籤，作為該群的預測類別
           labels[mask] = np.bincount(y_true[mask]).argmax()
       else:
           labels[mask] = 0  # 若該群沒資料，預設為正常類
   return labels

# 取得對齊後的預測標籤
y_pred_aligned = align_labels(y_test, y_pred_test, optimal_k)

def evaluation(y_true, y_pred, model_name="Model"):
   accuracy = accuracy_score(y_true, y_pred)
   precision = precision_score(y_true, y_pred, zero_division=0)
   recall = recall_score(y_true, y_pred)
   f1 = f1_score(y_true, y_pred)

   print(f'\n{model_name} Evaluation:')
   print('===' * 15)
   print('         Accuracy:', accuracy)
   print('  Precision Score:', precision)
   print('     Recall Score:', recall)
   print('         F1 Score:', f1)
   print("\nClassification Report:")
   print(classification_report(y_true, y_pred))

evaluation(y_test, y_pred_aligned, model_name="KMeans (Unsupervised)")