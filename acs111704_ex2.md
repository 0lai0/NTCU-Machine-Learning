# 🔍 Challenge 2 – Fraud Detection with Fusion Models

## 🎯 Objective:
To combine **unsupervised anomaly detection** (Isolation Forest) with **supervised classification** (XGBoost) for improved credit card fraud detection.

---

## 📚 Dataset:
- Dataset: `creditcard.csv`
- Source: Kaggle
- Records: 284,807 transactions
- Fraud ratio: ~0.17%
- Preprocessing:
  - Dropped `Time`
  - Scaled `Amount` with `StandardScaler`

---

## ⚙️ Methodology:

### 1. Isolation Forest (Unsupervised)
- Detected anomalies without using class labels.
- Output (-1 or 1) converted into a new binary column: `anomaly_score`.
- Treated as an **engineered feature**.

### 2. XGBoost Classifier (Supervised)
- Trained on the full dataset including the new `anomaly_score`.
- Used `train_test_split` (70%/30%) with stratified sampling.
- Evaluation via precision, recall, and F1-score.

---

## 📊 Results:

### 🔍 XGBoost with Anomaly Feature

c:\Users\Asus\AppData\Local\Programs\Python\Python312\Lib\site-packages\xgboost\training.py:183: UserWarning: [01:12:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
🧠 XGBoost with Anomaly Feature
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     85295
           1       0.93      0.73      0.82       148

    accuracy                           1.00     85443
   macro avg       0.97      0.86      0.91     85443
weighted avg       1.00      1.00      1.00     85443